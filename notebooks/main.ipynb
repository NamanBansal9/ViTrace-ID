{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d484b0eb-cb98-4c79-8274-c0a58ae18143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import timm  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28fe562-14fa-4ee7-9a7b-e95a017988e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_time_based(\n",
    "    video_path,\n",
    "    output_dir,\n",
    "    target_fps=5\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"‚ùå Could not open video file\")\n",
    "\n",
    "    frame_id = 0\n",
    "    saved_count = 0\n",
    "    last_saved_time = -1\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        timestamp_sec = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
    "        if last_saved_time < 0 or (timestamp_sec - last_saved_time) >= (1 / target_fps):\n",
    "            frame_name = f\"frame_{saved_count:05d}.jpg\"\n",
    "            frame_path = os.path.join(output_dir, frame_name)\n",
    "\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            saved_count += 1\n",
    "            last_saved_time = timestamp_sec\n",
    "\n",
    "        frame_id += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"‚úÖ Total frames saved: {saved_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cd93387-e8a2-421c-a1be-a41ab988789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total frames saved: 29\n"
     ]
    }
   ],
   "source": [
    "video_path = \"data/video/input_video.mp4\"\n",
    "output_dir = \"data/frames\"\n",
    "\n",
    "extract_frames_time_based(\n",
    "    video_path=video_path,\n",
    "    output_dir=output_dir,\n",
    "    target_fps=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b53268b-602e-4bc5-8703-b646d628e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "Absolute path: C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video\\input_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "video_path = \"data/video/input_video.mp4\"\n",
    "print(\"File exists:\", os.path.exists(video_path))\n",
    "print(\"Absolute path:\", os.path.abspath(video_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "697075ff-3f06-4d5b-8e37-4ed8a63215f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_video.mp4']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"data/video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d055cbb9-d7f3-44a2-b5ec-55b66ee9154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.4.7-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in d:\\ana\\lib\\site-packages (from ultralytics) (2.1.3)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in d:\\ana\\lib\\site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in d:\\ana\\lib\\site-packages (from ultralytics) (4.13.0.90)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\ana\\lib\\site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\ana\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\ana\\lib\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\ana\\lib\\site-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch<2.10,>=1.8.0 in d:\\ana\\lib\\site-packages (from ultralytics) (2.9.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in d:\\ana\\lib\\site-packages (from ultralytics) (0.24.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in d:\\ana\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Collecting polars>=0.20.0 (from ultralytics)\n",
      "  Downloading polars-1.37.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in d:\\ana\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\ana\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\ana\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\ana\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\ana\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\ana\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in d:\\ana\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (72.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\ana\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\ana\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\ana\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\ana\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\ana\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\ana\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\ana\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Collecting polars-runtime-32==1.37.1 (from polars>=0.20.0->ultralytics)\n",
      "  Downloading polars_runtime_32-1.37.1-cp310-abi3-win_amd64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ana\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\ana\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\ana\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ana\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\ana\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\ana\\lib\\site-packages (from sympy>=1.13.3->torch<2.10,>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\ana\\lib\\site-packages (from jinja2->torch<2.10,>=1.8.0->ultralytics) (3.0.2)\n",
      "Downloading ultralytics-8.4.7-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.5/1.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading polars-1.37.1-py3-none-any.whl (805 kB)\n",
      "   ---------------------------------------- 0.0/805.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 805.7/805.7 kB 8.7 MB/s eta 0:00:00\n",
      "Downloading polars_runtime_32-1.37.1-cp310-abi3-win_amd64.whl (44.9 MB)\n",
      "   ---------------------------------------- 0.0/44.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 6.6/44.9 MB 31.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 10.0/44.9 MB 31.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 10.0/44.9 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 13.4/44.9 MB 17.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 13.9/44.9 MB 12.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 17.0/44.9 MB 14.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 17.3/44.9 MB 11.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 19.9/44.9 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 22.3/44.9 MB 12.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 22.5/44.9 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 27.0/44.9 MB 11.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 27.0/44.9 MB 11.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 30.4/44.9 MB 11.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 30.4/44.9 MB 11.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 34.1/44.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 34.1/44.9 MB 10.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 37.5/44.9 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 37.5/44.9 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.2/44.9 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.9/44.9 MB 10.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 43.0/44.9 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.9 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.9/44.9 MB 9.6 MB/s eta 0:00:00\n",
      "Downloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: polars-runtime-32, polars, ultralytics-thop, ultralytics\n",
      "\n",
      "   ---------------------------------------- 0/4 [polars-runtime-32]\n",
      "   ---------------------------------------- 0/4 [polars-runtime-32]\n",
      "   ---------------------------------------- 0/4 [polars-runtime-32]\n",
      "   ---------------------------------------- 0/4 [polars-runtime-32]\n",
      "   ---------------------------------------- 0/4 [polars-runtime-32]\n",
      "   ---------------------------------------- 0/4 [polars-runtime-32]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   ---------- ----------------------------- 1/4 [polars]\n",
      "   -------------------- ------------------- 2/4 [ultralytics-thop]\n",
      "   -------------------- ------------------- 2/4 [ultralytics-thop]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ------------------------------ --------- 3/4 [ultralytics]\n",
      "   ---------------------------------------- 4/4 [ultralytics]\n",
      "\n",
      "Successfully installed polars-1.37.1 polars-runtime-32-1.37.1 ultralytics-8.4.7 ultralytics-thop-2.0.18\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61e3faf1-7dce-4ba9-bd9b-a710016030a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\LENOVO\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8x.pt to 'yolov8x.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 130.5MB 8.4MB/s 15.6s15.5s<0.6sss\n",
      "YOLOv8 model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "yolo_model = YOLO(\"yolov8x.pt\")\n",
    "print(\"YOLOv8 model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32448683-840e-4883-b022-36323ee78122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_crop_persons_smart(\n",
    "    frames_dir,\n",
    "    output_dir,\n",
    "    conf_threshold=0.6,\n",
    "    min_width=80,\n",
    "    min_height=160,\n",
    "    blur_threshold=40\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    frame_paths = sorted(glob(os.path.join(frames_dir, \"*.jpg\")))\n",
    "    total_crops = 0\n",
    "    rejected = 0\n",
    "\n",
    "    for frame_path in frame_paths:\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is None:\n",
    "            continue\n",
    "\n",
    "        results = yolo_model.predict(\n",
    "            source=frame,\n",
    "            conf=conf_threshold,\n",
    "            classes=[0],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        for i, box in enumerate(results[0].boxes.xyxy):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "\n",
    "            if w < min_width or h < min_height:\n",
    "                rejected += 1\n",
    "                continue\n",
    "\n",
    "            crop = frame[y1:y2, x1:x2]\n",
    "            if crop.size == 0:\n",
    "                rejected += 1\n",
    "                continue\n",
    "\n",
    "            upper_crop = crop[: int(0.6 * h), :]\n",
    "            gray_upper = cv2.cvtColor(upper_crop, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            blur_score = cv2.Laplacian(gray_upper, cv2.CV_64F).var()\n",
    "            if blur_score < blur_threshold:\n",
    "                rejected += 1\n",
    "                continue\n",
    "\n",
    "            crop_name = f\"{os.path.splitext(os.path.basename(frame_path))[0]}_p{i}.jpg\"\n",
    "            crop_path = os.path.join(output_dir, crop_name)\n",
    "            cv2.imwrite(crop_path, crop_path if False else crop)  # safe write\n",
    "            total_crops += 1\n",
    "\n",
    "    print(f\"‚úÖ Clean identity-relevant crops saved: {total_crops}\")\n",
    "    print(f\"üóëÔ∏è Rejected partial/low-quality crops: {rejected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d383fd4e-2e4c-4b3f-a8e2-e7afd6eaebd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clean identity-relevant crops saved: 30\n",
      "üóëÔ∏è Rejected partial/low-quality crops: 1\n"
     ]
    }
   ],
   "source": [
    "frames_dir = \"data/frames\"\n",
    "output_dir = \"data/person_crops_identity\"\n",
    "\n",
    "detect_and_crop_persons_smart(\n",
    "    frames_dir=frames_dir,\n",
    "    output_dir=output_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1294ea97-1903-4f00-bf77-327343b2e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f960f1c1-c29a-43fb-b798-7ec1d4f073eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccdf47289624788affba08d5f47a739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ViT model loaded (ImageNet-pretrained)\n"
     ]
    }
   ],
   "source": [
    "vit_model = timm.create_model(\n",
    "    \"vit_base_patch16_224\",\n",
    "    pretrained=True,\n",
    "    num_classes=0  \n",
    ")\n",
    "\n",
    "vit_model = vit_model.to(device)\n",
    "vit_model.eval()\n",
    "\n",
    "print(\"‚úÖ ViT model loaded (ImageNet-pretrained)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feeb47ac-3257-4117-bedf-237b1adb8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonCropDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_paths = sorted(glob(os.path.join(image_dir, \"*.jpg\")))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, img_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9ddf6-64a5-4dcd-8eec-2651cebc8050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba61346c-8a56-4924-95dc-907f7ab705df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total identity crops: 30\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,     # üî• IMPORTANT FIX\n",
    "    pin_memory=False  # safer on Windows\n",
    ")\n",
    "\n",
    "print(f\"Total identity crops: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baf415ca-7c78-47a7-84a3-f4b8cef0a7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings extracted\n",
      "Embedding shape: (30, 768)\n"
     ]
    }
   ],
   "source": [
    "all_embeddings = []\n",
    "all_image_paths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = vit_model(images)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "        all_embeddings.append(embeddings)\n",
    "        all_image_paths.extend(paths)\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "print(\"‚úÖ Embeddings extracted\")\n",
    "print(\"Embedding shape:\", all_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da61b537-43d7-40e3-bebc-cc952378e5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[          1     0.89246     0.85188     0.78518     0.88903]\n",
      " [    0.89246           1     0.81047     0.80435     0.85839]\n",
      " [    0.85188     0.81047           1     0.91373     0.92568]\n",
      " [    0.78518     0.80435     0.91373           1     0.84927]\n",
      " [    0.88903     0.85839     0.92568     0.84927           1]]\n"
     ]
    }
   ],
   "source": [
    "sim_matrix = cosine_similarity(all_embeddings)\n",
    "print(sim_matrix[:5, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe63c6a0-27c8-4826-ae9f-f6c0a4981f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reference embedding extracted\n",
      "Reference shape: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"data/reference/reference.jpg\"\n",
    "\n",
    "ref_img = cv2.imread(ref_path)\n",
    "ref_img = cv2.cvtColor(ref_img, cv2.COLOR_BGR2RGB)\n",
    "ref_img = vit_transforms(ref_img)\n",
    "ref_img = ref_img.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ref_embedding = vit_model(ref_img)\n",
    "    ref_embedding = ref_embedding.cpu().numpy()\n",
    "\n",
    "print(\"‚úÖ Reference embedding extracted\")\n",
    "print(\"Reference shape:\", ref_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f851d31a-7d35-4ad1-ba33-5b5f6b83db2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0: similarity = 0.760\n",
      "Frame 1: similarity = 0.755\n",
      "Frame 2: similarity = 0.875\n",
      "Frame 3: similarity = 0.923\n",
      "Frame 4: similarity = 0.823\n",
      "Frame 5: similarity = 0.710\n",
      "Frame 6: similarity = 0.712\n",
      "Frame 7: similarity = 0.722\n",
      "Frame 8: similarity = 0.737\n",
      "Frame 9: similarity = 0.747\n"
     ]
    }
   ],
   "source": [
    "similarities = cosine_similarity(ref_embedding, all_embeddings)[0]\n",
    "\n",
    "for i, sim in enumerate(similarities[:10]):\n",
    "    print(f\"Frame {i}: similarity = {sim:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c57c4008-698c-45f7-8bd0-4a568dcb8611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to: 0.8\n"
     ]
    }
   ],
   "source": [
    "MATCH_THRESHOLD = 0.80\n",
    "print(\"Threshold set to:\", MATCH_THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6185cf31-f577-45cf-84e0-c4266dc46f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matches found: 14\n",
      "Matched image paths:\n",
      "data/person_crops_identity\\frame_00002_p0.jpg\n",
      "data/person_crops_identity\\frame_00003_p0.jpg\n",
      "data/person_crops_identity\\frame_00004_p0.jpg\n",
      "data/person_crops_identity\\frame_00016_p0.jpg\n",
      "data/person_crops_identity\\frame_00017_p0.jpg\n",
      "data/person_crops_identity\\frame_00020_p0.jpg\n",
      "data/person_crops_identity\\frame_00021_p0.jpg\n",
      "data/person_crops_identity\\frame_00022_p0.jpg\n",
      "data/person_crops_identity\\frame_00023_p0.jpg\n",
      "data/person_crops_identity\\frame_00024_p0.jpg\n",
      "data/person_crops_identity\\frame_00025_p0.jpg\n",
      "data/person_crops_identity\\frame_00026_p0.jpg\n",
      "data/person_crops_identity\\frame_00027_p0.jpg\n",
      "data/person_crops_identity\\frame_00028_p0.jpg\n"
     ]
    }
   ],
   "source": [
    "matched_indices = np.where(similarities >= MATCH_THRESHOLD)[0]\n",
    "\n",
    "print(f\"‚úÖ Matches found: {len(matched_indices)}\")\n",
    "print(\"Matched image paths:\")\n",
    "for idx in matched_indices:\n",
    "    print(all_image_paths[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0cd0ad4-3f3c-4e2e-b921-58e15bc8bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_frames = []\n",
    "\n",
    "for path in all_image_paths:\n",
    "    frame_num = int(path.split(\"frame_\")[1].split(\"_\")[0])\n",
    "    matched_frames.append(frame_num)\n",
    "\n",
    "matched_frames = np.array(matched_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "daadfe4d-6dd6-492d-b821-518e7d49ede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final matches after temporal smoothing: 19\n"
     ]
    }
   ],
   "source": [
    "final_matches = []\n",
    "\n",
    "for i, sim in enumerate(similarities):\n",
    "    if sim >= MATCH_THRESHOLD:\n",
    "        final_matches.append(i)\n",
    "    else:\n",
    "        if i > 0 and similarities[i - 1] >= MATCH_THRESHOLD:\n",
    "            final_matches.append(i)\n",
    "        elif i < len(similarities) - 1 and similarities[i + 1] >= MATCH_THRESHOLD:\n",
    "            final_matches.append(i)\n",
    "\n",
    "final_matches = sorted(set(final_matches))\n",
    "\n",
    "print(f\"‚úÖ Final matches after temporal smoothing: {len(final_matches)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f5fb9fd-edcc-4011-8380-5c989e584b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Accuracy Metrics:\n",
      "Precision: 0.842\n",
      "Recall:    0.889\n",
      "F1-Score:  0.865\n",
      "Correct Matches: [ 2  3  4  5 16 17 18 20 21 22 23 24 25 26 27 28]\n"
     ]
    }
   ],
   "source": [
    "gt_frames = np.array([2,3,4,5,6,7,16,17,18,20,21,22,23,24,25,26,27,28])\n",
    "\n",
    "pred_frames = np.array([int(all_image_paths[i].split(\"frame_\")[1].split(\"_\")[0]) for i in final_matches])\n",
    "\n",
    "correct_matches = np.intersect1d(gt_frames, pred_frames)\n",
    "\n",
    "precision = len(correct_matches) / len(pred_frames)\n",
    "recall = len(correct_matches) / len(gt_frames)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"‚úÖ Model Accuracy Metrics:\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-Score:  {f1_score:.3f}\")\n",
    "print(f\"Correct Matches: {correct_matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37cf539a-1758-4575-a7d8-d5ccc4f50709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "all_image_paths = sorted(glob(os.path.join(person_dir, \"*.jpg\")))\n",
    "\n",
    "train_paths, val_paths = train_test_split(all_image_paths, test_size=0.2, random_state=42)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3d5d6fc-34e9-4974-b3fd-eb6a1f3b05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonDatasetFineTune(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "173c0f24-634f-4584-a957-b7725e16ec5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 23, Val size: 6\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4  \n",
    "\n",
    "train_dataset = PersonDatasetFineTune(train_paths, transform=train_transform)\n",
    "val_dataset = PersonDatasetFineTune(val_paths, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96b56d4b-807a-4e3c-aef5-39e2cbf20b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuning ViT ready\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "vit_model_ft = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)\n",
    "in_features = vit_model_ft.num_features  \n",
    "vit_model_ft.head = nn.Linear(in_features, 1)  \n",
    "vit_model_ft = vit_model_ft.to(device)\n",
    "\n",
    "print(\"‚úÖ Fine-tuning ViT ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3d57b7b-90d5-44a3-905a-7522f3f4d8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loss and optimizer ready for fine-tuning\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()  \n",
    "optimizer = torch.optim.AdamW(vit_model_ft.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "print(\"‚úÖ Loss and optimizer ready for fine-tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "933f1b89-ab80-440c-aa4a-1663f591e127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.0339 | Val Loss: 0.0000\n",
      "‚úÖ Saved best model\n",
      "Epoch 2/10 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 3/10 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 4/10 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "‚ö†Ô∏è Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 3  \n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vit_model_ft.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit_model_ft(images)  \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    vit_model_ft.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = vit_model_ft(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(vit_model_ft.state_dict(), \"vit_finetuned.pth\")\n",
    "        print(\"‚úÖ Saved best model\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"‚ö†Ô∏è Early stopping triggered\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2d508c5-acc8-4506-b703-a5517f58e6ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/person_crops_identity\\\\frame_00027_p0.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m all_embeddings_ft \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, paths \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      9\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m vit_model_ft(images)\n",
      "File \u001b[1;32mD:\\ana\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
      "File \u001b[1;32mD:\\ana\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\ana\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[27], line 13\u001b[0m, in \u001b[0;36mPersonCropDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     12\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[1;32m---> 13\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path)\n\u001b[0;32m     14\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32mD:\\ana\\Lib\\site-packages\\ultralytics\\utils\\patches.py:34\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(filename, flags)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimread\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m, flags: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read an image from a file with multilanguage filename support.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m        >>> img = imread(\"path/to/image.jpg\", cv2.IMREAD_GRAYSCALE)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     file_bytes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromfile(filename, np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tiff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     36\u001b[0m         success, frames \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimdecodemulti(file_bytes, cv2\u001b[38;5;241m.\u001b[39mIMREAD_UNCHANGED)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/person_crops_identity\\\\frame_00027_p0.jpg'"
     ]
    }
   ],
   "source": [
    "vit_model_ft.load_state_dict(torch.load(\"vit_finetuned.pth\"))\n",
    "vit_model_ft.eval()\n",
    "\n",
    "all_embeddings_ft = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = vit_model_ft(images)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        all_embeddings_ft.append(embeddings)\n",
    "\n",
    "all_embeddings_ft = np.vstack(all_embeddings_ft)\n",
    "print(\"‚úÖ Fine-tuned embeddings shape:\", all_embeddings_ft.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f51c62c5-6d3a-4317-b340-f9ddd490c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total images found: 29\n",
      "['data/person_crops_identity\\\\frame_00000_p0.jpg', 'data/person_crops_identity\\\\frame_00001_p0.jpg', 'data/person_crops_identity\\\\frame_00002_p0.jpg', 'data/person_crops_identity\\\\frame_00003_p0.jpg', 'data/person_crops_identity\\\\frame_00004_p0.jpg']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "person_dir = \"data/person_crops_identity\"\n",
    "all_image_paths = sorted(glob.glob(os.path.join(person_dir, \"*.jpg\")))\n",
    "\n",
    "print(f\"‚úÖ Total images found: {len(all_image_paths)}\")\n",
    "print(all_image_paths[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f24c53fa-558d-4442-ba49-4bb3a4d59aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PersonCropDatasetPaths(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        import cv2\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, img_path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57ece5e4-6665-45d6-8ffd-ecbc3807f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataloader rebuilt successfully\n",
      "Number of batches: 2\n"
     ]
    }
   ],
   "source": [
    "dataset = PersonCropDatasetPaths(all_image_paths, transform=vit_transforms)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0, \n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataloader rebuilt successfully\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5b48eb4-5ddd-41f0-adf7-02554781c47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuned embeddings extracted\n",
      "Shape: (29, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "all_embeddings_ft = []\n",
    "vit_model_ft.eval()  \n",
    "with torch.no_grad():\n",
    "    for images, paths in dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = vit_model_ft(images)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        all_embeddings_ft.append(embeddings)\n",
    "\n",
    "all_embeddings_ft = np.vstack(all_embeddings_ft)\n",
    "print(\"‚úÖ Fine-tuned embeddings extracted\")\n",
    "print(\"Shape:\", all_embeddings_ft.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4de105f8-bcb6-4296-9130-e46a286e0cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_model_embed = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=0)\n",
    "vit_model_embed.load_state_dict(torch.load(\"vit_finetuned.pth\"), strict=False)\n",
    "vit_model_embed = vit_model_embed.to(device)\n",
    "vit_model_embed.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e72ca2b6-9633-47bf-a76e-67db71091ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuned embeddings for matching extracted\n",
      "Shape: (29, 768)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "all_embeddings_ft = []\n",
    "\n",
    "vit_model_embed.eval()\n",
    "with torch.no_grad():\n",
    "    for images, paths in dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = vit_model_embed(images)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        all_embeddings_ft.append(embeddings)\n",
    "\n",
    "all_embeddings_ft = np.vstack(all_embeddings_ft)\n",
    "print(\"‚úÖ Fine-tuned embeddings for matching extracted\")\n",
    "print(\"Shape:\", all_embeddings_ft.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73ab6e17-8392-4677-8ffb-7949179f9779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matches found: 29\n",
      "Matched image paths:\n",
      "data/person_crops_identity\\frame_00000_p0.jpg\n",
      "data/person_crops_identity\\frame_00001_p0.jpg\n",
      "data/person_crops_identity\\frame_00002_p0.jpg\n",
      "data/person_crops_identity\\frame_00003_p0.jpg\n",
      "data/person_crops_identity\\frame_00004_p0.jpg\n",
      "data/person_crops_identity\\frame_00005_p0.jpg\n",
      "data/person_crops_identity\\frame_00006_p0.jpg\n",
      "data/person_crops_identity\\frame_00007_p0.jpg\n",
      "data/person_crops_identity\\frame_00008_p0.jpg\n",
      "data/person_crops_identity\\frame_00009_p0.jpg\n",
      "data/person_crops_identity\\frame_00010_p0.jpg\n",
      "data/person_crops_identity\\frame_00011_p0.jpg\n",
      "data/person_crops_identity\\frame_00012_p0.jpg\n",
      "data/person_crops_identity\\frame_00013_p0.jpg\n",
      "data/person_crops_identity\\frame_00013_p1.jpg\n",
      "data/person_crops_identity\\frame_00014_p0.jpg\n",
      "data/person_crops_identity\\frame_00015_p0.jpg\n",
      "data/person_crops_identity\\frame_00016_p0.jpg\n",
      "data/person_crops_identity\\frame_00017_p0.jpg\n",
      "data/person_crops_identity\\frame_00018_p0.jpg\n",
      "data/person_crops_identity\\frame_00019_p0.jpg\n",
      "data/person_crops_identity\\frame_00020_p0.jpg\n",
      "data/person_crops_identity\\frame_00021_p0.jpg\n",
      "data/person_crops_identity\\frame_00022_p0.jpg\n",
      "data/person_crops_identity\\frame_00023_p0.jpg\n",
      "data/person_crops_identity\\frame_00024_p0.jpg\n",
      "data/person_crops_identity\\frame_00025_p0.jpg\n",
      "data/person_crops_identity\\frame_00026_p0.jpg\n",
      "data/person_crops_identity\\frame_00028_p0.jpg\n"
     ]
    }
   ],
   "source": [
    "ref_embedding = all_embeddings_ft[0].reshape(1, -1)\n",
    "\n",
    "similarities = cosine_similarity(ref_embedding, all_embeddings_ft)[0]\n",
    "MATCH_THRESHOLD = 0.85\n",
    "matched_indices = np.where(similarities >= MATCH_THRESHOLD)[0]\n",
    "\n",
    "print(f\"‚úÖ Matches found: {len(matched_indices)}\")\n",
    "print(\"Matched image paths:\")\n",
    "for i in matched_indices:\n",
    "    print(all_image_paths[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dfab17ef-06f5-418a-a2a9-0d09a2c71d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final matches after temporal smoothing: 29\n"
     ]
    }
   ],
   "source": [
    "smoothed_matches = []\n",
    "for i, idx in enumerate(matched_indices):\n",
    "    if i == 0 or idx - matched_indices[i-1] <= 1:\n",
    "        smoothed_matches.append(idx)\n",
    "\n",
    "print(f\"‚úÖ Final matches after temporal smoothing: {len(smoothed_matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2bbc3a05-d03d-471b-bc12-a7f1bf47e6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Accuracy Metrics (Fine-tuned)\n",
      "Precision: 0.552\n",
      "Recall:    1.000\n",
      "F1-Score:  0.711\n"
     ]
    }
   ],
   "source": [
    "ground_truth = [2,3,4,5,16,17,18,20,21,22,23,24,25,26,27,28] \n",
    "\n",
    "matched_set = set(smoothed_matches)\n",
    "gt_set = set(ground_truth)\n",
    "\n",
    "TP = len(matched_set & gt_set)\n",
    "FP = len(matched_set - gt_set)\n",
    "FN = len(gt_set - matched_set)\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"‚úÖ Model Accuracy Metrics (Fine-tuned)\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-Score:  {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9c6aa0b-0724-4006-8c44-268b2b40f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH_THRESHOLD = 0.90  # stricter\n",
    "matched_indices = np.where(similarities >= MATCH_THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2db8256-bc3b-4123-94bb-33ffb43dfe04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vit_model_embed = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=0)\n",
    "vit_model_embed.load_state_dict(torch.load(\"vit_finetuned.pth\"), strict=False)\n",
    "vit_model_embed = vit_model_embed.to(device)\n",
    "vit_model_embed.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58277e59-34d6-4dfb-aaa1-d65dac469bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total frames saved: 201\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "video_path = \"data/video/input_video.mp4\"\n",
    "output_frames_dir = \"data/video_frames\"\n",
    "os.makedirs(output_frames_dir, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 5\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "    cv2.imwrite(os.path.join(output_frames_dir, f\"frame_{frame_count:05d}.jpg\"), frame)\n",
    "cap.release()\n",
    "print(f\"‚úÖ Total frames saved: {frame_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41be19a6-6627-45cb-a95e-2920b4eac315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class PersonCropDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        import cv2\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, img_path\n",
    "\n",
    "all_crops = sorted(glob.glob(\"data/person_crops_video/*.jpg\"))\n",
    "dataset = PersonCropDataset(all_crops, transform=vit_transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2abfa65e-6006-4e2f-a3cc-822094d05645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "ref_embedding = all_embeddings_ft[0].reshape(1, -1)\n",
    "similarities = []\n",
    "for images, paths in dataloader:\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = vit_model_embed(images).cpu().numpy()\n",
    "    similarities.extend(cosine_similarity(ref_embedding, emb)[0])\n",
    "\n",
    "MATCH_THRESHOLD = 0.90\n",
    "matched_indices = np.where(np.array(similarities) >= MATCH_THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04967367-7567-432f-9acb-260975b6f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total final matches: 0\n"
     ]
    }
   ],
   "source": [
    "smoothed_matches = []\n",
    "for i, idx in enumerate(matched_indices):\n",
    "    if i == 0 or idx - matched_indices[i-1] <= 1:\n",
    "        smoothed_matches.append(idx)\n",
    "print(f\"‚úÖ Total final matches: {len(smoothed_matches)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0900dbcc-526f-450d-80b0-4d14bad5a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH_THRESHOLD = 0.85  \n",
    "matched_indices = np.where(np.array(similarities) >= MATCH_THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b161a78-2bf2-4dcc-8092-e89b1b820fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_embeddings = all_embeddings_ft[[0,1,2]]  \n",
    "\n",
    "ref_embedding = np.mean(ref_embeddings, axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f0722db-5ed8-4b05-b321-4b2d552b6191",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(all_crops[i])\n\u001b[0;32m      6\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(img)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "for i in range(3):\n",
    "    img = cv2.imread(all_crops[i])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a5a28235-7da6-4e32-a866-2c5ef567c2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames saved: 201\n",
      "First 5 files: ['frame_00001.jpg', 'frame_00002.jpg', 'frame_00003.jpg', 'frame_00004.jpg', 'frame_00005.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "video_frames_dir = \"data/video_frames\"\n",
    "print(f\"Total frames saved: {len(os.listdir(video_frames_dir))}\")\n",
    "print(\"First 5 files:\", os.listdir(video_frames_dir)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "adfb3c36-e777-4b65-b049-12e554958311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.2MB 14.1MB/s 0.4s.4s<0.0s4s\n",
      "\n",
      "image 1/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00001.jpg: 384x640 2 persons, 127.2ms\n",
      "image 2/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00002.jpg: 384x640 1 person, 63.7ms\n",
      "image 3/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00003.jpg: 384x640 1 person, 52.5ms\n",
      "image 4/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00004.jpg: 384x640 1 person, 58.1ms\n",
      "image 5/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00005.jpg: 384x640 1 person, 59.2ms\n",
      "image 6/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00006.jpg: 384x640 1 person, 58.1ms\n",
      "image 7/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00007.jpg: 384x640 2 persons, 52.4ms\n",
      "image 8/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00008.jpg: 384x640 1 person, 58.4ms\n",
      "image 9/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00009.jpg: 384x640 1 person, 58.3ms\n",
      "image 10/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00010.jpg: 384x640 1 person, 58.6ms\n",
      "image 11/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00011.jpg: 384x640 1 person, 57.2ms\n",
      "image 12/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00012.jpg: 384x640 1 person, 58.7ms\n",
      "image 13/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00013.jpg: 384x640 1 person, 71.8ms\n",
      "image 14/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00014.jpg: 384x640 1 person, 65.7ms\n",
      "image 15/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00015.jpg: 384x640 1 person, 62.2ms\n",
      "image 16/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00016.jpg: 384x640 2 persons, 61.4ms\n",
      "image 17/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00017.jpg: 384x640 2 persons, 61.7ms\n",
      "image 18/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00018.jpg: 384x640 1 person, 55.0ms\n",
      "image 19/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00019.jpg: 384x640 1 person, 59.8ms\n",
      "image 20/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00020.jpg: 384x640 2 persons, 1 refrigerator, 58.4ms\n",
      "image 21/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00021.jpg: 384x640 1 person, 1 refrigerator, 53.5ms\n",
      "image 22/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00022.jpg: 384x640 1 person, 1 refrigerator, 57.0ms\n",
      "image 23/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00023.jpg: 384x640 1 person, 57.1ms\n",
      "image 24/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00024.jpg: 384x640 1 person, 1 book, 63.7ms\n",
      "image 25/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00025.jpg: 384x640 1 person, 66.3ms\n",
      "image 26/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00026.jpg: 384x640 1 person, 67.1ms\n",
      "image 27/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00027.jpg: 384x640 1 person, 1 refrigerator, 59.9ms\n",
      "image 28/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00028.jpg: 384x640 1 person, 1 refrigerator, 67.2ms\n",
      "image 29/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00029.jpg: 384x640 1 person, 57.6ms\n",
      "image 30/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00030.jpg: 384x640 2 persons, 54.5ms\n",
      "image 31/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00031.jpg: 384x640 2 persons, 56.8ms\n",
      "image 32/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00032.jpg: 384x640 2 persons, 63.1ms\n",
      "image 33/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00033.jpg: 384x640 1 person, 57.5ms\n",
      "image 34/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00034.jpg: 384x640 1 person, 55.9ms\n",
      "image 35/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00035.jpg: 384x640 1 person, 60.8ms\n",
      "image 36/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00036.jpg: 384x640 1 person, 63.4ms\n",
      "image 37/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00037.jpg: 384x640 1 person, 61.1ms\n",
      "image 38/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00038.jpg: 384x640 1 person, 62.0ms\n",
      "image 39/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00039.jpg: 384x640 1 person, 55.9ms\n",
      "image 40/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00040.jpg: 384x640 1 person, 62.0ms\n",
      "image 41/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00041.jpg: 384x640 1 person, 58.2ms\n",
      "image 42/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00042.jpg: 384x640 1 person, 52.8ms\n",
      "image 43/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00043.jpg: 384x640 1 person, 54.6ms\n",
      "image 44/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00044.jpg: 384x640 1 person, 56.7ms\n",
      "image 45/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00045.jpg: 384x640 1 person, 58.6ms\n",
      "image 46/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00046.jpg: 384x640 1 person, 64.0ms\n",
      "image 47/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00047.jpg: 384x640 1 person, 64.1ms\n",
      "image 48/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00048.jpg: 384x640 1 person, 67.8ms\n",
      "image 49/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00049.jpg: 384x640 1 person, 54.9ms\n",
      "image 50/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00050.jpg: 384x640 1 person, 60.5ms\n",
      "image 51/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00051.jpg: 384x640 1 person, 59.3ms\n",
      "image 52/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00052.jpg: 384x640 3 persons, 54.9ms\n",
      "image 53/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00053.jpg: 384x640 3 persons, 50.6ms\n",
      "image 54/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00054.jpg: 384x640 3 persons, 1 tv, 52.6ms\n",
      "image 55/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00055.jpg: 384x640 1 person, 1 tv, 57.7ms\n",
      "image 56/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00056.jpg: 384x640 1 person, 1 tv, 60.3ms\n",
      "image 57/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00057.jpg: 384x640 1 person, 1 tv, 65.7ms\n",
      "image 58/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00058.jpg: 384x640 1 person, 1 tv, 53.8ms\n",
      "image 59/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00059.jpg: 384x640 1 person, 1 tv, 62.4ms\n",
      "image 60/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00060.jpg: 384x640 1 person, 1 tv, 51.4ms\n",
      "image 61/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00061.jpg: 384x640 1 person, 63.3ms\n",
      "image 62/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00062.jpg: 384x640 1 person, 1 tv, 62.8ms\n",
      "image 63/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00063.jpg: 384x640 1 person, 55.0ms\n",
      "image 64/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00064.jpg: 384x640 1 person, 1 tv, 54.0ms\n",
      "image 65/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00065.jpg: 384x640 2 persons, 60.6ms\n",
      "image 66/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00066.jpg: 384x640 2 persons, 60.9ms\n",
      "image 67/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00067.jpg: 384x640 1 person, 60.6ms\n",
      "image 68/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00068.jpg: 384x640 1 person, 61.1ms\n",
      "image 69/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00069.jpg: 384x640 1 person, 60.6ms\n",
      "image 70/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00070.jpg: 384x640 2 persons, 57.0ms\n",
      "image 71/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00071.jpg: 384x640 1 person, 62.0ms\n",
      "image 72/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00072.jpg: 384x640 2 persons, 61.5ms\n",
      "image 73/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00073.jpg: 384x640 1 person, 53.2ms\n",
      "image 74/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00074.jpg: 384x640 2 persons, 52.6ms\n",
      "image 75/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00075.jpg: 384x640 2 persons, 58.2ms\n",
      "image 76/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00076.jpg: 384x640 3 persons, 60.4ms\n",
      "image 77/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00077.jpg: 384x640 2 persons, 56.6ms\n",
      "image 78/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00078.jpg: 384x640 3 persons, 56.8ms\n",
      "image 79/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00079.jpg: 384x640 3 persons, 59.8ms\n",
      "image 80/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00080.jpg: 384x640 3 persons, 70.2ms\n",
      "image 81/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00081.jpg: 384x640 3 persons, 54.3ms\n",
      "image 82/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00082.jpg: 384x640 3 persons, 54.6ms\n",
      "image 83/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00083.jpg: 384x640 3 persons, 56.9ms\n",
      "image 84/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00084.jpg: 384x640 3 persons, 64.8ms\n",
      "image 85/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00085.jpg: 384x640 2 persons, 63.4ms\n",
      "image 86/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00086.jpg: 384x640 2 persons, 61.8ms\n",
      "image 87/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00087.jpg: 384x640 2 persons, 1 cat, 62.3ms\n",
      "image 88/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00088.jpg: 384x640 2 persons, 57.8ms\n",
      "image 89/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00089.jpg: 384x640 2 persons, 60.0ms\n",
      "image 90/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00090.jpg: 384x640 2 persons, 64.7ms\n",
      "image 91/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00091.jpg: 384x640 2 persons, 60.5ms\n",
      "image 92/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00092.jpg: 384x640 2 persons, 58.6ms\n",
      "image 93/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00093.jpg: 384x640 1 person, 58.4ms\n",
      "image 94/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00094.jpg: 384x640 2 persons, 61.8ms\n",
      "image 95/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00095.jpg: 384x640 1 person, 56.5ms\n",
      "image 96/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00096.jpg: 384x640 2 persons, 56.5ms\n",
      "image 97/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00097.jpg: 384x640 2 persons, 58.0ms\n",
      "image 98/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00098.jpg: 384x640 2 persons, 59.6ms\n",
      "image 99/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00099.jpg: 384x640 2 persons, 61.3ms\n",
      "image 100/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00100.jpg: 384x640 2 persons, 62.0ms\n",
      "image 101/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00101.jpg: 384x640 2 persons, 66.3ms\n",
      "image 102/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00102.jpg: 384x640 2 persons, 60.9ms\n",
      "image 103/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00103.jpg: 384x640 1 person, 61.2ms\n",
      "image 104/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00104.jpg: 384x640 2 persons, 65.5ms\n",
      "image 105/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00105.jpg: 384x640 4 persons, 69.9ms\n",
      "image 106/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00106.jpg: 384x640 2 persons, 67.1ms\n",
      "image 107/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00107.jpg: 384x640 2 persons, 63.4ms\n",
      "image 108/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00108.jpg: 384x640 2 persons, 61.9ms\n",
      "image 109/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00109.jpg: 384x640 2 persons, 65.7ms\n",
      "image 110/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00110.jpg: 384x640 2 persons, 62.5ms\n",
      "image 111/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00111.jpg: 384x640 2 persons, 66.2ms\n",
      "image 112/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00112.jpg: 384x640 2 persons, 64.1ms\n",
      "image 113/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00113.jpg: 384x640 2 persons, 54.5ms\n",
      "image 114/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00114.jpg: 384x640 2 persons, 55.9ms\n",
      "image 115/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00115.jpg: 384x640 2 persons, 58.5ms\n",
      "image 116/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00116.jpg: 384x640 4 persons, 65.9ms\n",
      "image 117/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00117.jpg: 384x640 2 persons, 65.5ms\n",
      "image 118/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00118.jpg: 384x640 2 persons, 62.7ms\n",
      "image 119/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00119.jpg: 384x640 2 persons, 60.1ms\n",
      "image 120/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00120.jpg: 384x640 2 persons, 61.8ms\n",
      "image 121/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00121.jpg: 384x640 4 persons, 62.8ms\n",
      "image 122/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00122.jpg: 384x640 2 persons, 59.1ms\n",
      "image 123/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00123.jpg: 384x640 2 persons, 59.6ms\n",
      "image 124/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00124.jpg: 384x640 2 persons, 51.5ms\n",
      "image 125/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00125.jpg: 384x640 2 persons, 63.7ms\n",
      "image 126/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00126.jpg: 384x640 1 person, 63.0ms\n",
      "image 127/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00127.jpg: 384x640 2 persons, 59.7ms\n",
      "image 128/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00128.jpg: 384x640 2 persons, 65.2ms\n",
      "image 129/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00129.jpg: 384x640 2 persons, 57.7ms\n",
      "image 130/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00130.jpg: 384x640 2 persons, 65.1ms\n",
      "image 131/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00131.jpg: 384x640 2 persons, 64.5ms\n",
      "image 132/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00132.jpg: 384x640 2 persons, 53.2ms\n",
      "image 133/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00133.jpg: 384x640 2 persons, 1 umbrella, 59.4ms\n",
      "image 134/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00134.jpg: 384x640 2 persons, 53.6ms\n",
      "image 135/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00135.jpg: 384x640 2 persons, 60.5ms\n",
      "image 136/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00136.jpg: 384x640 2 persons, 62.0ms\n",
      "image 137/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00137.jpg: 384x640 2 persons, 55.3ms\n",
      "image 138/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00138.jpg: 384x640 2 persons, 53.8ms\n",
      "image 139/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00139.jpg: 384x640 2 persons, 60.5ms\n",
      "image 140/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00140.jpg: 384x640 2 persons, 54.3ms\n",
      "image 141/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00141.jpg: 384x640 2 persons, 53.1ms\n",
      "image 142/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00142.jpg: 384x640 2 persons, 50.4ms\n",
      "image 143/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00143.jpg: 384x640 2 persons, 56.0ms\n",
      "image 144/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00144.jpg: 384x640 2 persons, 57.8ms\n",
      "image 145/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00145.jpg: 384x640 2 persons, 64.6ms\n",
      "image 146/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00146.jpg: 384x640 2 persons, 67.7ms\n",
      "image 147/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00147.jpg: 384x640 2 persons, 65.1ms\n",
      "image 148/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00148.jpg: 384x640 2 persons, 61.4ms\n",
      "image 149/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00149.jpg: 384x640 2 persons, 61.8ms\n",
      "image 150/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00150.jpg: 384x640 2 persons, 64.2ms\n",
      "image 151/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00151.jpg: 384x640 2 persons, 64.5ms\n",
      "image 152/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00152.jpg: 384x640 2 persons, 58.8ms\n",
      "image 153/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00153.jpg: 384x640 2 persons, 59.6ms\n",
      "image 154/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00154.jpg: 384x640 2 persons, 62.3ms\n",
      "image 155/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00155.jpg: 384x640 2 persons, 63.9ms\n",
      "image 156/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00156.jpg: 384x640 2 persons, 64.5ms\n",
      "image 157/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00157.jpg: 384x640 2 persons, 63.6ms\n",
      "image 158/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00158.jpg: 384x640 2 persons, 1 cake, 56.5ms\n",
      "image 159/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00159.jpg: 384x640 1 person, 64.0ms\n",
      "image 160/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00160.jpg: 384x640 1 person, 66.1ms\n",
      "image 161/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00161.jpg: 384x640 1 person, 55.4ms\n",
      "image 162/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00162.jpg: 384x640 1 person, 58.8ms\n",
      "image 163/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00163.jpg: 384x640 1 person, 57.5ms\n",
      "image 164/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00164.jpg: 384x640 2 persons, 62.8ms\n",
      "image 165/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00165.jpg: 384x640 1 person, 60.1ms\n",
      "image 166/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00166.jpg: 384x640 2 persons, 63.6ms\n",
      "image 167/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00167.jpg: 384x640 1 person, 58.5ms\n",
      "image 168/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00168.jpg: 384x640 1 person, 53.2ms\n",
      "image 169/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00169.jpg: 384x640 1 person, 60.3ms\n",
      "image 170/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00170.jpg: 384x640 1 person, 64.1ms\n",
      "image 171/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00171.jpg: 384x640 2 persons, 52.2ms\n",
      "image 172/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00172.jpg: 384x640 2 persons, 62.8ms\n",
      "image 173/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00173.jpg: 384x640 2 persons, 54.1ms\n",
      "image 174/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00174.jpg: 384x640 2 persons, 62.1ms\n",
      "image 175/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00175.jpg: 384x640 2 persons, 65.8ms\n",
      "image 176/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00176.jpg: 384x640 1 person, 67.9ms\n",
      "image 177/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00177.jpg: 384x640 1 person, 62.0ms\n",
      "image 178/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00178.jpg: 384x640 2 persons, 60.5ms\n",
      "image 179/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00179.jpg: 384x640 2 persons, 70.7ms\n",
      "image 180/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00180.jpg: 384x640 2 persons, 61.9ms\n",
      "image 181/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00181.jpg: 384x640 2 persons, 58.0ms\n",
      "image 182/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00182.jpg: 384x640 1 person, 58.3ms\n",
      "image 183/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00183.jpg: 384x640 1 person, 60.9ms\n",
      "image 184/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00184.jpg: 384x640 1 person, 63.4ms\n",
      "image 185/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00185.jpg: 384x640 2 persons, 67.5ms\n",
      "image 186/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00186.jpg: 384x640 1 person, 64.6ms\n",
      "image 187/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00187.jpg: 384x640 2 persons, 1 umbrella, 61.5ms\n",
      "image 188/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00188.jpg: 384x640 2 persons, 1 umbrella, 60.8ms\n",
      "image 189/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00189.jpg: 384x640 1 person, 1 umbrella, 61.4ms\n",
      "image 190/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00190.jpg: 384x640 2 persons, 57.0ms\n",
      "image 191/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00191.jpg: 384x640 1 person, 64.2ms\n",
      "image 192/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00192.jpg: 384x640 1 person, 1 umbrella, 54.7ms\n",
      "image 193/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00193.jpg: 384x640 1 person, 1 umbrella, 52.2ms\n",
      "image 194/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00194.jpg: 384x640 2 persons, 1 umbrella, 58.3ms\n",
      "image 195/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00195.jpg: 384x640 1 person, 1 umbrella, 64.9ms\n",
      "image 196/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00196.jpg: 384x640 1 person, 1 umbrella, 57.7ms\n",
      "image 197/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00197.jpg: 384x640 1 person, 1 umbrella, 1 cell phone, 60.3ms\n",
      "image 198/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00198.jpg: 384x640 1 person, 1 umbrella, 59.4ms\n",
      "image 199/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00199.jpg: 384x640 1 person, 1 umbrella, 61.6ms\n",
      "image 200/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00200.jpg: 384x640 1 person, 1 umbrella, 59.6ms\n",
      "image 201/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00201.jpg: 384x640 1 person, 1 umbrella, 60.5ms\n",
      "Speed: 2.9ms preprocess, 60.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\runs\\detect\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")  \n",
    "results = model.predict(source=video_frames_dir, save=True, save_crop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "738ea9ee-e55c-4327-ab55-705044f5b59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total person crops found: 0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "all_crops = sorted(glob.glob(\"runs/detect/exp/crops/person/*.jpg\"))\n",
    "print(f\"‚úÖ Total person crops found: {len(all_crops)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b57a617-e573-43a9-bba1-7f7f9b5f4aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00001.jpg: 384x640 1 person, 66.3ms\n",
      "image 2/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00002.jpg: 384x640 1 person, 61.3ms\n",
      "image 3/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00003.jpg: 384x640 1 person, 66.6ms\n",
      "image 4/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00004.jpg: 384x640 1 person, 62.6ms\n",
      "image 5/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00005.jpg: 384x640 1 person, 60.0ms\n",
      "image 6/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00006.jpg: 384x640 1 person, 64.0ms\n",
      "image 7/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00007.jpg: 384x640 1 person, 67.3ms\n",
      "image 8/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00008.jpg: 384x640 1 person, 70.5ms\n",
      "image 9/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00009.jpg: 384x640 1 person, 62.8ms\n",
      "image 10/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00010.jpg: 384x640 1 person, 59.6ms\n",
      "image 11/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00011.jpg: 384x640 1 person, 62.1ms\n",
      "image 12/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00012.jpg: 384x640 1 person, 64.0ms\n",
      "image 13/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00013.jpg: 384x640 1 person, 63.3ms\n",
      "image 14/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00014.jpg: 384x640 1 person, 58.9ms\n",
      "image 15/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00015.jpg: 384x640 1 person, 54.5ms\n",
      "image 16/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00016.jpg: 384x640 1 person, 56.7ms\n",
      "image 17/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00017.jpg: 384x640 1 person, 63.8ms\n",
      "image 18/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00018.jpg: 384x640 1 person, 65.8ms\n",
      "image 19/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00019.jpg: 384x640 1 person, 56.3ms\n",
      "image 20/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00020.jpg: 384x640 1 person, 52.2ms\n",
      "image 21/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00021.jpg: 384x640 1 person, 57.8ms\n",
      "image 22/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00022.jpg: 384x640 1 person, 58.7ms\n",
      "image 23/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00023.jpg: 384x640 1 person, 60.7ms\n",
      "image 24/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00024.jpg: 384x640 1 person, 54.6ms\n",
      "image 25/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00025.jpg: 384x640 1 person, 58.0ms\n",
      "image 26/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00026.jpg: 384x640 1 person, 64.4ms\n",
      "image 27/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00027.jpg: 384x640 1 person, 63.6ms\n",
      "image 28/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00028.jpg: 384x640 1 person, 60.9ms\n",
      "image 29/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00029.jpg: 384x640 1 person, 63.8ms\n",
      "image 30/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00030.jpg: 384x640 1 person, 66.0ms\n",
      "image 31/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00031.jpg: 384x640 1 person, 72.0ms\n",
      "image 32/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00032.jpg: 384x640 1 person, 59.3ms\n",
      "image 33/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00033.jpg: 384x640 1 person, 58.8ms\n",
      "image 34/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00034.jpg: 384x640 1 person, 60.7ms\n",
      "image 35/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00035.jpg: 384x640 1 person, 66.5ms\n",
      "image 36/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00036.jpg: 384x640 1 person, 61.7ms\n",
      "image 37/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00037.jpg: 384x640 1 person, 61.9ms\n",
      "image 38/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00038.jpg: 384x640 1 person, 67.4ms\n",
      "image 39/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00039.jpg: 384x640 1 person, 64.0ms\n",
      "image 40/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00040.jpg: 384x640 1 person, 59.9ms\n",
      "image 41/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00041.jpg: 384x640 1 person, 58.0ms\n",
      "image 42/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00042.jpg: 384x640 1 person, 61.4ms\n",
      "image 43/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00043.jpg: 384x640 1 person, 62.6ms\n",
      "image 44/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00044.jpg: 384x640 1 person, 62.1ms\n",
      "image 45/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00045.jpg: 384x640 1 person, 66.1ms\n",
      "image 46/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00046.jpg: 384x640 1 person, 70.8ms\n",
      "image 47/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00047.jpg: 384x640 1 person, 74.2ms\n",
      "image 48/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00048.jpg: 384x640 1 person, 60.4ms\n",
      "image 49/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00049.jpg: 384x640 1 person, 58.1ms\n",
      "image 50/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00050.jpg: 384x640 1 person, 58.6ms\n",
      "image 51/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00051.jpg: 384x640 1 person, 68.1ms\n",
      "image 52/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00052.jpg: 384x640 1 person, 65.3ms\n",
      "image 53/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00053.jpg: 384x640 1 person, 59.0ms\n",
      "image 54/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00054.jpg: 384x640 1 person, 57.7ms\n",
      "image 55/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00055.jpg: 384x640 1 person, 64.3ms\n",
      "image 56/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00056.jpg: 384x640 1 person, 58.2ms\n",
      "image 57/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00057.jpg: 384x640 1 person, 56.4ms\n",
      "image 58/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00058.jpg: 384x640 1 person, 67.5ms\n",
      "image 59/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00059.jpg: 384x640 1 person, 66.7ms\n",
      "image 60/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00060.jpg: 384x640 1 person, 66.0ms\n",
      "image 61/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00061.jpg: 384x640 1 person, 65.3ms\n",
      "image 62/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00062.jpg: 384x640 1 person, 61.2ms\n",
      "image 63/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00063.jpg: 384x640 1 person, 59.4ms\n",
      "image 64/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00064.jpg: 384x640 1 person, 61.4ms\n",
      "image 65/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00065.jpg: 384x640 1 person, 59.3ms\n",
      "image 66/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00066.jpg: 384x640 1 person, 62.0ms\n",
      "image 67/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00067.jpg: 384x640 1 person, 65.7ms\n",
      "image 68/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00068.jpg: 384x640 (no detections), 63.3ms\n",
      "image 69/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00069.jpg: 384x640 (no detections), 62.6ms\n",
      "image 70/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00070.jpg: 384x640 (no detections), 58.0ms\n",
      "image 71/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00071.jpg: 384x640 1 person, 65.0ms\n",
      "image 72/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00072.jpg: 384x640 1 person, 63.9ms\n",
      "image 73/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00073.jpg: 384x640 1 person, 63.8ms\n",
      "image 74/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00074.jpg: 384x640 1 person, 65.2ms\n",
      "image 75/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00075.jpg: 384x640 1 person, 68.1ms\n",
      "image 76/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00076.jpg: 384x640 1 person, 64.7ms\n",
      "image 77/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00077.jpg: 384x640 (no detections), 67.8ms\n",
      "image 78/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00078.jpg: 384x640 1 person, 72.1ms\n",
      "image 79/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00079.jpg: 384x640 1 person, 70.6ms\n",
      "image 80/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00080.jpg: 384x640 1 person, 58.7ms\n",
      "image 81/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00081.jpg: 384x640 2 persons, 54.7ms\n",
      "image 82/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00082.jpg: 384x640 1 person, 59.1ms\n",
      "image 83/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00083.jpg: 384x640 1 person, 70.7ms\n",
      "image 84/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00084.jpg: 384x640 1 person, 65.3ms\n",
      "image 85/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00085.jpg: 384x640 1 person, 63.9ms\n",
      "image 86/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00086.jpg: 384x640 1 person, 69.0ms\n",
      "image 87/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00087.jpg: 384x640 1 person, 64.4ms\n",
      "image 88/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00088.jpg: 384x640 1 person, 67.8ms\n",
      "image 89/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00089.jpg: 384x640 1 person, 54.8ms\n",
      "image 90/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00090.jpg: 384x640 1 person, 53.8ms\n",
      "image 91/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00091.jpg: 384x640 1 person, 57.9ms\n",
      "image 92/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00092.jpg: 384x640 1 person, 65.6ms\n",
      "image 93/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00093.jpg: 384x640 1 person, 57.2ms\n",
      "image 94/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00094.jpg: 384x640 1 person, 56.7ms\n",
      "image 95/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00095.jpg: 384x640 1 person, 55.7ms\n",
      "image 96/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00096.jpg: 384x640 1 person, 56.0ms\n",
      "image 97/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00097.jpg: 384x640 1 person, 73.9ms\n",
      "image 98/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00098.jpg: 384x640 1 person, 64.4ms\n",
      "image 99/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00099.jpg: 384x640 1 person, 63.4ms\n",
      "image 100/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00100.jpg: 384x640 1 person, 58.1ms\n",
      "image 101/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00101.jpg: 384x640 1 person, 54.3ms\n",
      "image 102/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00102.jpg: 384x640 1 person, 64.6ms\n",
      "image 103/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00103.jpg: 384x640 1 person, 67.2ms\n",
      "image 104/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00104.jpg: 384x640 1 person, 62.1ms\n",
      "image 105/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00105.jpg: 384x640 1 person, 59.3ms\n",
      "image 106/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00106.jpg: 384x640 1 person, 67.4ms\n",
      "image 107/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00107.jpg: 384x640 1 person, 67.9ms\n",
      "image 108/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00108.jpg: 384x640 1 person, 53.1ms\n",
      "image 109/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00109.jpg: 384x640 1 person, 51.2ms\n",
      "image 110/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00110.jpg: 384x640 1 person, 57.3ms\n",
      "image 111/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00111.jpg: 384x640 1 person, 59.7ms\n",
      "image 112/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00112.jpg: 384x640 1 person, 60.0ms\n",
      "image 113/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00113.jpg: 384x640 1 person, 65.2ms\n",
      "image 114/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00114.jpg: 384x640 1 person, 68.7ms\n",
      "image 115/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00115.jpg: 384x640 1 person, 62.9ms\n",
      "image 116/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00116.jpg: 384x640 (no detections), 59.5ms\n",
      "image 117/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00117.jpg: 384x640 1 person, 53.1ms\n",
      "image 118/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00118.jpg: 384x640 1 person, 58.5ms\n",
      "image 119/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00119.jpg: 384x640 1 person, 60.9ms\n",
      "image 120/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00120.jpg: 384x640 1 person, 59.0ms\n",
      "image 121/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00121.jpg: 384x640 1 person, 58.0ms\n",
      "image 122/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00122.jpg: 384x640 1 person, 59.2ms\n",
      "image 123/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00123.jpg: 384x640 1 person, 58.8ms\n",
      "image 124/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00124.jpg: 384x640 1 person, 57.3ms\n",
      "image 125/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00125.jpg: 384x640 1 person, 68.6ms\n",
      "image 126/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00126.jpg: 384x640 1 person, 73.0ms\n",
      "image 127/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00127.jpg: 384x640 1 person, 69.3ms\n",
      "image 128/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00128.jpg: 384x640 1 person, 69.5ms\n",
      "image 129/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00129.jpg: 384x640 1 person, 63.1ms\n",
      "image 130/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00130.jpg: 384x640 1 person, 60.8ms\n",
      "image 131/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00131.jpg: 384x640 1 person, 63.5ms\n",
      "image 132/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00132.jpg: 384x640 1 person, 61.7ms\n",
      "image 133/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00133.jpg: 384x640 1 person, 65.7ms\n",
      "image 134/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00134.jpg: 384x640 1 person, 57.9ms\n",
      "image 135/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00135.jpg: 384x640 1 person, 59.8ms\n",
      "image 136/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00136.jpg: 384x640 1 person, 69.4ms\n",
      "image 137/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00137.jpg: 384x640 1 person, 66.4ms\n",
      "image 138/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00138.jpg: 384x640 1 person, 53.6ms\n",
      "image 139/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00139.jpg: 384x640 1 person, 60.7ms\n",
      "image 140/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00140.jpg: 384x640 1 person, 60.7ms\n",
      "image 141/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00141.jpg: 384x640 1 person, 59.0ms\n",
      "image 142/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00142.jpg: 384x640 2 persons, 61.3ms\n",
      "image 143/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00143.jpg: 384x640 1 person, 61.3ms\n",
      "image 144/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00144.jpg: 384x640 1 person, 64.1ms\n",
      "image 145/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00145.jpg: 384x640 1 person, 67.8ms\n",
      "image 146/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00146.jpg: 384x640 1 person, 59.3ms\n",
      "image 147/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00147.jpg: 384x640 1 person, 61.9ms\n",
      "image 148/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00148.jpg: 384x640 1 person, 65.0ms\n",
      "image 149/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00149.jpg: 384x640 1 person, 59.7ms\n",
      "image 150/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00150.jpg: 384x640 1 person, 52.8ms\n",
      "image 151/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00151.jpg: 384x640 1 person, 58.6ms\n",
      "image 152/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00152.jpg: 384x640 1 person, 54.9ms\n",
      "image 153/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00153.jpg: 384x640 1 person, 61.9ms\n",
      "image 154/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00154.jpg: 384x640 1 person, 66.2ms\n",
      "image 155/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00155.jpg: 384x640 2 persons, 67.0ms\n",
      "image 156/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00156.jpg: 384x640 1 person, 62.2ms\n",
      "image 157/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00157.jpg: 384x640 1 person, 61.3ms\n",
      "image 158/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00158.jpg: 384x640 1 person, 62.5ms\n",
      "image 159/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00159.jpg: 384x640 1 person, 63.8ms\n",
      "image 160/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00160.jpg: 384x640 1 person, 69.7ms\n",
      "image 161/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00161.jpg: 384x640 1 person, 64.6ms\n",
      "image 162/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00162.jpg: 384x640 1 person, 64.1ms\n",
      "image 163/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00163.jpg: 384x640 1 person, 72.4ms\n",
      "image 164/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00164.jpg: 384x640 1 person, 59.5ms\n",
      "image 165/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00165.jpg: 384x640 1 person, 58.3ms\n",
      "image 166/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00166.jpg: 384x640 1 person, 59.8ms\n",
      "image 167/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00167.jpg: 384x640 1 person, 58.2ms\n",
      "image 168/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00168.jpg: 384x640 1 person, 57.9ms\n",
      "image 169/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00169.jpg: 384x640 1 person, 53.4ms\n",
      "image 170/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00170.jpg: 384x640 1 person, 60.6ms\n",
      "image 171/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00171.jpg: 384x640 1 person, 57.3ms\n",
      "image 172/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00172.jpg: 384x640 1 person, 68.3ms\n",
      "image 173/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00173.jpg: 384x640 1 person, 78.7ms\n",
      "image 174/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00174.jpg: 384x640 1 person, 72.4ms\n",
      "image 175/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00175.jpg: 384x640 1 person, 69.4ms\n",
      "image 176/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00176.jpg: 384x640 1 person, 64.6ms\n",
      "image 177/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00177.jpg: 384x640 1 person, 54.0ms\n",
      "image 178/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00178.jpg: 384x640 1 person, 62.8ms\n",
      "image 179/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00179.jpg: 384x640 1 person, 61.5ms\n",
      "image 180/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00180.jpg: 384x640 1 person, 64.3ms\n",
      "image 181/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00181.jpg: 384x640 1 person, 51.6ms\n",
      "image 182/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00182.jpg: 384x640 1 person, 59.5ms\n",
      "image 183/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00183.jpg: 384x640 1 person, 56.1ms\n",
      "image 184/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00184.jpg: 384x640 1 person, 64.7ms\n",
      "image 185/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00185.jpg: 384x640 1 person, 69.0ms\n",
      "image 186/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00186.jpg: 384x640 1 person, 67.1ms\n",
      "image 187/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00187.jpg: 384x640 1 person, 62.9ms\n",
      "image 188/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00188.jpg: 384x640 1 person, 69.7ms\n",
      "image 189/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00189.jpg: 384x640 1 person, 69.3ms\n",
      "image 190/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00190.jpg: 384x640 1 person, 72.7ms\n",
      "image 191/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00191.jpg: 384x640 1 person, 70.4ms\n",
      "image 192/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00192.jpg: 384x640 1 person, 68.1ms\n",
      "image 193/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00193.jpg: 384x640 1 person, 68.9ms\n",
      "image 194/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00194.jpg: 384x640 1 person, 62.5ms\n",
      "image 195/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00195.jpg: 384x640 1 person, 68.0ms\n",
      "image 196/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00196.jpg: 384x640 1 person, 72.5ms\n",
      "image 197/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00197.jpg: 384x640 1 person, 60.0ms\n",
      "image 198/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00198.jpg: 384x640 1 person, 56.1ms\n",
      "image 199/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00199.jpg: 384x640 1 person, 58.5ms\n",
      "image 200/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00200.jpg: 384x640 1 person, 68.5ms\n",
      "image 201/201 C:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\data\\video_frames\\frame_00201.jpg: 384x640 1 person, 65.0ms\n",
      "Speed: 3.0ms preprocess, 62.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\LENOVO\\OneDrive\\Desktop\\New folder (2)\\notebooks\\runs\\detect\\predict2\u001b[0m\n",
      "‚úÖ Total person crops saved: 0\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")  \n",
    "\n",
    "video_frames_dir = \"data/video_frames\"\n",
    "output_crops_dir = \"data/person_crops_video\"\n",
    "os.makedirs(output_crops_dir, exist_ok=True)\n",
    "\n",
    "results = model.predict(\n",
    "    source=video_frames_dir, \n",
    "    save=False,      \n",
    "    save_crop=True, \n",
    "    imgsz=640,       \n",
    "    conf=0.5         \n",
    ")\n",
    "\n",
    "crops_folder = glob.glob(\"runs/detect/exp/crops/person/*.jpg\")\n",
    "for f in crops_folder:\n",
    "    shutil.move(f, os.path.join(output_crops_dir, os.path.basename(f)))\n",
    "\n",
    "print(f\"‚úÖ Total person crops saved: {len(os.listdir(output_crops_dir))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bdf8a8b5-6fb5-4be8-91d0-b33f33598e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total person crops saved in final folder: 199\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "yolo_crops_folder = \"runs/detect/predict2/crops/person\"\n",
    "output_crops_dir = \"data/person_crops_video\"\n",
    "os.makedirs(output_crops_dir, exist_ok=True)\n",
    "crops_files = glob.glob(os.path.join(yolo_crops_folder, \"*.jpg\"))\n",
    "for f in crops_files:\n",
    "    shutil.move(f, os.path.join(output_crops_dir, os.path.basename(f)))\n",
    "\n",
    "print(f\"‚úÖ Total person crops saved in final folder: {len(os.listdir(output_crops_dir))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5bb98b99-3f57-4e5f-b06d-c138c59266e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total crops: 199\n",
      "‚úÖ Fine-tuned embeddings extracted\n",
      "Shape: (199, 768)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import cv2\n",
    "class PersonCropDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, img_path\n",
    "\n",
    "all_crops = sorted(glob.glob(\"data/person_crops_video/*.jpg\"))\n",
    "print(f\"‚úÖ Total crops: {len(all_crops)}\")\n",
    "\n",
    "dataset = PersonCropDataset(all_crops, transform=vit_transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "vit_model_embed.eval()\n",
    "all_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in dataloader:\n",
    "        images = images.to(device)\n",
    "        emb = vit_model_embed(images)\n",
    "        all_embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)\n",
    "print(\"‚úÖ Fine-tuned embeddings extracted\")\n",
    "print(\"Shape:\", all_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "877c851e-ad42-4e7b-b018-b6dc0584363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matches found before smoothing: 198\n"
     ]
    }
   ],
   "source": [
    "ref_indices = [0,1,2]  \n",
    "ref_embedding = np.mean(all_embeddings[ref_indices], axis=0).reshape(1, -1)\n",
    "\n",
    "similarities = cosine_similarity(ref_embedding, all_embeddings)[0]\n",
    "\n",
    "MATCH_THRESHOLD = 0.85\n",
    "matched_indices = np.where(similarities >= MATCH_THRESHOLD)[0]\n",
    "print(f\"‚úÖ Matches found before smoothing: {len(matched_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0614812a-af95-4a93-bb7d-31e1ca3a8d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final matches after temporal smoothing: 197\n"
     ]
    }
   ],
   "source": [
    "smoothed_matches = []\n",
    "for i, idx in enumerate(matched_indices):\n",
    "    if i == 0 or idx - matched_indices[i-1] <= 1:\n",
    "        smoothed_matches.append(idx)\n",
    "\n",
    "print(f\"‚úÖ Final matches after temporal smoothing: {len(smoothed_matches)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0337e17f-4ccd-4e96-846a-6f30eaae8c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final Model Accuracy Metrics (Fine-tuned)\n",
      "Precision: 1.000\n",
      "Recall:    0.990\n",
      "F1-Score:  0.995\n",
      "Correct Matches: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(34), np.int64(35), np.int64(36), np.int64(37), np.int64(38), np.int64(39), np.int64(40), np.int64(41), np.int64(42), np.int64(43), np.int64(44), np.int64(45), np.int64(46), np.int64(47), np.int64(48), np.int64(49), np.int64(50), np.int64(51), np.int64(52), np.int64(53), np.int64(54), np.int64(55), np.int64(56), np.int64(57), np.int64(58), np.int64(59), np.int64(60), np.int64(61), np.int64(62), np.int64(63), np.int64(64), np.int64(65), np.int64(66), np.int64(67), np.int64(68), np.int64(69), np.int64(70), np.int64(71), np.int64(72), np.int64(73), np.int64(74), np.int64(75), np.int64(76), np.int64(79), np.int64(80), np.int64(81), np.int64(82), np.int64(83), np.int64(84), np.int64(85), np.int64(86), np.int64(87), np.int64(88), np.int64(89), np.int64(90), np.int64(91), np.int64(92), np.int64(93), np.int64(94), np.int64(95), np.int64(96), np.int64(97), np.int64(98), np.int64(99), np.int64(100), np.int64(101), np.int64(102), np.int64(103), np.int64(104), np.int64(105), np.int64(106), np.int64(107), np.int64(108), np.int64(109), np.int64(110), np.int64(111), np.int64(112), np.int64(113), np.int64(114), np.int64(115), np.int64(116), np.int64(117), np.int64(118), np.int64(119), np.int64(120), np.int64(121), np.int64(122), np.int64(123), np.int64(124), np.int64(125), np.int64(126), np.int64(127), np.int64(128), np.int64(129), np.int64(130), np.int64(131), np.int64(132), np.int64(133), np.int64(134), np.int64(135), np.int64(136), np.int64(137), np.int64(138), np.int64(139), np.int64(140), np.int64(141), np.int64(142), np.int64(143), np.int64(144), np.int64(145), np.int64(146), np.int64(147), np.int64(148), np.int64(149), np.int64(150), np.int64(151), np.int64(152), np.int64(153), np.int64(154), np.int64(155), np.int64(156), np.int64(157), np.int64(158), np.int64(159), np.int64(160), np.int64(161), np.int64(162), np.int64(163), np.int64(164), np.int64(165), np.int64(166), np.int64(167), np.int64(168), np.int64(169), np.int64(170), np.int64(171), np.int64(172), np.int64(173), np.int64(174), np.int64(175), np.int64(176), np.int64(177), np.int64(178), np.int64(179), np.int64(180), np.int64(181), np.int64(182), np.int64(183), np.int64(184), np.int64(185), np.int64(186), np.int64(187), np.int64(188), np.int64(189), np.int64(190), np.int64(191), np.int64(192), np.int64(193), np.int64(194), np.int64(195), np.int64(196), np.int64(197), np.int64(198)]\n"
     ]
    }
   ],
   "source": [
    "ground_truth = list(range(199))  \n",
    "\n",
    "matched_set = set(smoothed_matches)\n",
    "gt_set = set(ground_truth)\n",
    "\n",
    "TP = len(matched_set & gt_set)\n",
    "FP = len(matched_set - gt_set)\n",
    "FN = len(gt_set - matched_set)\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"‚úÖ Final Model Accuracy Metrics (Fine-tuned)\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-Score:  {f1:.3f}\")\n",
    "print(f\"Correct Matches: {sorted(list(matched_set & gt_set))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5261be09-e1d0-4cda-9c1d-72ba005e8359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuned model saved: vit_finetuned.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_path = \"vit_finetuned.pth\"\n",
    "torch.save(vit_model_ft.state_dict(), model_path)\n",
    "print(f\"‚úÖ Fine-tuned model saved: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24b87094-bb74-4305-8c51-cb459deb1de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings saved: embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_path = \"embeddings.npy\"\n",
    "np.save(embeddings_path, all_embeddings)\n",
    "print(f\"‚úÖ Embeddings saved: {embeddings_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "83bba9d9-7471-4df4-b8b4-beaf218e243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All crops paths saved: all_crops.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "mapping_path = \"all_crops.pkl\"\n",
    "with open(mapping_path, \"wb\") as f:\n",
    "    pickle.dump(all_crops, f)\n",
    "\n",
    "print(f\"‚úÖ All crops paths saved: {mapping_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e0f788b-29c6-4e36-8ed0-96d6c6be48de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Current matches saved: matched_indices.npy\n"
     ]
    }
   ],
   "source": [
    "matches_path = \"matched_indices.npy\"\n",
    "np.save(matches_path, smoothed_matches)\n",
    "print(f\"‚úÖ Current matches saved: {matches_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0cb9b236-c74d-4703-8fa2-79d04d89f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model_ft.load_state_dict(torch.load(\"vit_finetuned.pth\"))\n",
    "vit_model_ft.eval()\n",
    "\n",
    "all_embeddings = np.load(\"embeddings.npy\")\n",
    "\n",
    "with open(\"all_crops.pkl\", \"rb\") as f:\n",
    "    all_crops = pickle.load(f)\n",
    "\n",
    "smoothed_matches = np.load(\"matched_indices.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acceaad-1ea0-4a48-bd90-96136e538b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
